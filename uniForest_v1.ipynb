{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "uniForest_v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRU8kbuo7WTL"
      },
      "source": [
        "**uniForest *v*.1**\n",
        "---\n",
        "\n",
        "\n",
        "**Citation:**\n",
        "\n",
        "Leigh R.J., Murphy R.A., and Walsh F. (2021) Paper title, Journal metrics\n",
        "\n",
        "\n",
        "**Description:**\n",
        "\n",
        "uniForest is a user-friendly script for outlier processing in microbiome studies "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiVZPoU3ISDa"
      },
      "source": [
        "**Usage**\n",
        "\n",
        "**Please read the manual**\n",
        "\n",
        "---\n",
        "\n",
        "Each executable cell can be ran using the black mouseover to the left of their titles (denoted in **bold**). Simply upload a file, set your desired parameters, and run cells. Some cells display a lot of messages. These can be hidden after they're produced using the black mouseover in their output cell. \n",
        "\n",
        "After running the \"Data processing\" cell, an imputed dataset, change log, and descriptive statistics table are available to download using the **folder icon on the left side of the screen.**\n",
        "\n",
        "\n",
        "The code underlying each cell can be inspected by double clicking a given cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlsZ5x2JYEAY",
        "cellView": "form"
      },
      "source": [
        "#@title **Library installation**\n",
        "#@markdown This process installs specific Python library versions required to run this script. Several messages will be produced.\n",
        "\n",
        "!pip install scikit-learn==0.24 scikit-bio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY6Z5CAYYLtG",
        "cellView": "form"
      },
      "source": [
        "#@title **Import Python modules for data processing**\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import plotly.express as px\n",
        "import scipy.stats as stats\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import preprocessing\n",
        "from skbio.stats.distance import anosim, permanova\n",
        "from skbio.diversity import beta_diversity as beta_div\n",
        "#from google.colab import files\n",
        "import io\n",
        "import itertools"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhBKWH1VyHta",
        "cellView": "form",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "d725369e-dbdf-4c36-a095-ddd8b957fe3a"
      },
      "source": [
        "#@title **File upload**\n",
        "#@markdown A single tsv file is required for upload. This file must have taxa in a single column on the left and a single row on top labelled \"Groups\" (in cell A1). The \"Groups\" row specifies what group a given sample belongs to (eg. Lung_pretreatment or Soil_arctic). All samples of a given group should be beside each other.\n",
        "from google.colab import files\n",
        "dataset = files.upload()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c1c8e731-042e-4153-bf0e-356041358358\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c1c8e731-042e-4153-bf0e-356041358358\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving human3.tsv to human3 (1).tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qIRHaC4x7ZP",
        "cellView": "form"
      },
      "source": [
        "#@title **Metric specifications**\n",
        "#@markdown This cell defines the impution metric, whether data is to be scaled, whether Isolation Forest (iForest) bootstrapping is to be performed and the stringency of iForest.\n",
        "\n",
        "#@markdown Data **scaling** ensures all samples have an equivalent sum. This is to mitigate errors due to data with low coverage/depth.\n",
        "\n",
        "#@markdown iForest **bootstrapping** can be turned on with **\"True**\". Default usage is **\"False\"**.\n",
        "\n",
        "#@markdown iForest stringency is set using **contamination**. This value can be set between 0 (least stringent) and 0.5 (most stringent). Numerical values are set **without quotation marks**. The iForest algorithm can set this value automatically for each sample using **\"auto\"**. The default is **\"auto\"** (**with quotation marks**).\n",
        "\n",
        "#@markdown \n",
        "\n",
        "scale_data = \"yes\"\n",
        "iForest_contamination=\"auto\"\n",
        "iForest_bootstrap=\"False\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nTKJK1qLVAi",
        "cellView": "form"
      },
      "source": [
        "#@title **Data processing**\n",
        "datasetName = next(iter(dataset))\n",
        "dataset_df = pd.read_csv(io.BytesIO(dataset[datasetName]), sep='\\t', header=None).set_index(0)\n",
        "GroupColours = dataset_df.iloc[0].tolist()\n",
        "\n",
        "t = set(GroupColours)\n",
        "c = sorted(set(itertools.combinations(t, 2)))\n",
        "\n",
        "GroupedData = dataset_df.T.groupby(\"Group\", axis=0)\n",
        "Groups = set(GroupColours)\n",
        "#Metric = metric\n",
        "\n",
        "\n",
        "def Metric(group):\n",
        "  if metric == \"mean\":\n",
        "    Metric = np.mean(group)\n",
        "  if metric == \"median\":\n",
        "    Metric = np.median(group)\n",
        "  if metric == \"geometric\":\n",
        "    Metric = float(stats.gmean(group).item(0))\n",
        "  if metric == \"harmonic\":\n",
        "     Metric=float(stats.hmean(group).item(0))\n",
        "  return Metric\n",
        "\n",
        "def PairwiseDescriptiveStats(Group1, Group2):\n",
        "  L, S = stats.levene(Group1, Group2)\n",
        "  results = [np.mean(Group1), np.std(Group1), np.var(Group1), np.median(Group1), stats.hmean(Group1).item(0), stats.gmean(Group1).item(0), min(Group1), max(Group1), np.mean(Group2), np.std(Group2), np.var(Group2), np.median(Group2), stats.hmean(Group2).item(0), stats.gmean(Group2).item(0), min(Group2), max(Group2), L, S]\n",
        "  return results\n",
        "\n",
        "def Scale(item):\n",
        "    scaler = item.astype(float).sum(axis=0)/item.astype(float).sum(axis=0).max()\n",
        "    scaledData = item.astype(float)/scaler\n",
        "    scaledInputData = scaledData.values.tolist()\n",
        "    return scaledData, scaledInputData\n",
        "\n",
        "\n",
        "def iForest():\n",
        "  import warnings\n",
        "  warnings.filterwarnings('ignore') #This turns off the annoying numpy and scipy warnings\n",
        "  imputedDataset = pd.DataFrame()\n",
        "  changeLog = []\n",
        "\n",
        "  '''\n",
        "  The next section performs iForest and imputes outliers\n",
        "  '''\n",
        "\n",
        "  for Group in Groups:\n",
        "    List_Imputed = []\n",
        "    rawDataset = GroupedData.get_group(Group).set_index(\"Group\")\n",
        "    outliersRemoved_list = []\n",
        "    imputedDataset_list = []\n",
        "\n",
        "    for taxonID, dataSeries in rawDataset.iteritems():\n",
        "      imputedSeries = []\n",
        "      rawSeries = np.array(pd.Series(dataSeries).astype(np.float)).reshape(-1,1)\n",
        "      clf = IsolationForest(contamination=iForest_contamination, bootstrap=iForest_bootstrap, random_state=42)\n",
        "      preds = clf.fit_predict(rawSeries)\n",
        "      outliersRemovedSeries = rawSeries[np.where(preds == 1, True, False)]\n",
        "      if len(outliersRemovedSeries) != 0:\n",
        "        imputer = float(Metric(outliersRemovedSeries))\n",
        "        for item in rawSeries:\n",
        "          if item in outliersRemovedSeries:\n",
        "            imputedSeries.append(float(item))\n",
        "          else:\n",
        "            imputedSeries.append(imputer)\n",
        "      else:\n",
        "        imputedSeries = list(np.concatenate(rawSeries).flat)\n",
        "      imputedDataset_list.append(imputedSeries)\n",
        "    #List_imputed = pd.DataFrame.from_records(np.array(imputedDataset_list).T, index=rawDataset.index, columns=dataset_df.index.drop(\"Group\"))\n",
        "    #imputedDataset = imputedDataset.append(List_imputed) #.reindex(List_imputed.index)\n",
        "     \n",
        "      '''\n",
        "      The next section computes statistics for raw data, inlier data, and imputed data\n",
        "\n",
        "      '''\n",
        "     \n",
        "      raw_descriptiveStatistics = [np.mean(rawSeries), np.std(rawSeries), \n",
        "                                   np.median(rawSeries), np.var(rawSeries), \n",
        "                                   stats.hmean(np.float64(rawSeries)).item(0), stats.gmean(np.float64(rawSeries)).item(0), \n",
        "                                   stats.variation(rawSeries).item(0), \n",
        "                                   min(rawSeries), max(rawSeries)]\n",
        "      inlier_descriptiveStatistics = [np.mean(outliersRemovedSeries), np.std(outliersRemovedSeries), \n",
        "                                      np.median(outliersRemovedSeries), np.var(outliersRemovedSeries), \n",
        "                                      stats.hmean(np.float64(outliersRemovedSeries)).item(0), stats.gmean(np.float64(outliersRemovedSeries)).item(0), \n",
        "                                      stats.variation(outliersRemovedSeries).item(0), \n",
        "                                      min(rawSeries), max(rawSeries)]\n",
        "      imputed_descriptiveStatistics = [np.mean(imputedSeries), np.std(imputedSeries), \n",
        "                                       np.median(imputedSeries), np.var(imputedSeries), \n",
        "                                       stats.hmean(np.float64(imputedSeries)).item(0), stats.gmean(np.float64(imputedSeries)).item(0), \n",
        "                                       stats.variation(imputedSeries).item(0), \n",
        "                                       min(rawSeries), max(rawSeries)]\n",
        "\n",
        "      raw_descriptiveStatistics = list(np.float_(raw_descriptiveStatistics))\n",
        "      inlier_descriptiveStatistics = list(np.float_(inlier_descriptiveStatistics))\n",
        "      imputed_descriptiveStatistics = list(np.float_(imputed_descriptiveStatistics))\n",
        "      Levenes_W, Levenes_P = stats.levene(np.concatenate(rawSeries).flat, imputedSeries)\n",
        "\n",
        "      numberInliers = len(outliersRemovedSeries)\n",
        "      numberDataset = len(rawSeries)\n",
        "      numberOutliers = numberDataset-numberInliers\n",
        "      proportionInliers = numberOutliers/numberDataset\n",
        "      varianceReduction = 100-((np.var(imputedSeries)/np.var(rawSeries)*100))\n",
        "      CoVreduction = 100-((stats.variation(imputedSeries)/stats.variation(rawSeries)*100))\n",
        "      \n",
        "      descriptiveStatistics = [Group, taxonID, numberDataset, numberInliers, numberOutliers, proportionInliers] + raw_descriptiveStatistics + inlier_descriptiveStatistics + imputed_descriptiveStatistics + [float(varianceReduction), float(CoVreduction), float(Levenes_W), float(Levenes_P)]\n",
        "      \n",
        "      changeLog.append(descriptiveStatistics)\n",
        "\n",
        "    List_imputed = pd.DataFrame.from_records(np.array(imputedDataset_list).T, index=rawDataset.index, columns=dataset_df.index.drop(\"Group\"))  \n",
        "    imputedDataset = imputedDataset.append(List_imputed)\n",
        "\n",
        "  transposed_imputedDataset = imputedDataset.sort_index().T\n",
        "  grouped_imputedDataset = imputedDataset.groupby(\"Group\",axis=0)\n",
        "  changeStatistics = pd.DataFrame.from_records(changeLog, columns = [\"GroupID\", \"taxonID\", \"DatasetSize\", \"InliersSize\", \"OutliersSize\", \"OutliersProprotion\", \"Mean\", \"StandardDeviation\", \"Median\", \"Variance\", \"HarmonicMean\", \"GeometricMean\", \"Coefficient_of_Variation\", \"Minimum\", \"Maximum\", \"Mean\", \"StandardDeviation\", \"Median\", \"Variance\", \"HarmonicMean\", \"GeometricMean\", \"Coefficient_of_Variation\", \"Minimum\", \"Maximum\", \"Mean\", \"StandardDeviation\", \"Median\", \"Variance\", \"HarmonicMean\", \"GeometricMean\", \"Coefficient_of_Variation\", \"Minimum\", \"Maximum\", \"VarianceReduction\", \"CoV_Reduction\", \"Levenes_W\", \"Levenes_P\"])\n",
        "  return changeStatistics, imputedDataset, transposed_imputedDataset, grouped_imputedDataset\n",
        "\n",
        "\n",
        "\n",
        "def Components(item, figHeight, figWidth):\n",
        "  df = pd.DataFrame.from_records(item) #.drop(0, axis=1)\n",
        "  pca = PCA()\n",
        "  scaled_data = preprocessing.scale(df.T)\n",
        "  components = pca.fit_transform(scaled_data)\n",
        "  per_var = np.round(pca.explained_variance_ratio_* 100, decimals=2)\n",
        "  fig = px.scatter_3d(\n",
        "      components, x=0, y=1, z=2, color=Colours,\n",
        "      labels={'0': 'PC1 - {0}%'.format(per_var[0]), \n",
        "              '1': 'PC2 - {0}%'.format(per_var[1]), \n",
        "              '2': 'PC3 - {0}%'.format(per_var[2])}\n",
        "    )\n",
        "\n",
        "  fig.update_layout(margin=dict(l=1, r=1, b=1, t=1), autosize=True, width=figWidth,height=figHeight)\n",
        "  fig.update_layout(legend=dict(\n",
        "    yanchor=\"top\",\n",
        "     y=0.99,\n",
        "    xanchor=\"left\",\n",
        "    x=0.01))\n",
        "  \n",
        "  fig.show()\n",
        "  return   \n",
        "\n",
        "def BetaDiversity(Data, GroupIDs):\n",
        "  bc = beta_div(\"braycurtis\", Data, validate=True)\n",
        "  anosim_result = anosim(bc, GroupIDs, permutations=0)\n",
        "  stat, pvalue = anosim_result['test statistic'], anosim_result['p-value']\n",
        "  return bc, stat, pvalue\n",
        "\n",
        "def separationDistancesBetweenGroups(GroupedData, ImputedGroupData):\n",
        "  Results = []\n",
        "  t = set(GroupColours)\n",
        "  c = sorted(set(itertools.combinations(t, 2)))\n",
        "  for item in c:\n",
        "    raw_GroupA = GroupedData.get_group(item[0]).set_index(\"Group\")\n",
        "    raw_GroupB = GroupedData.get_group(item[1]).set_index(\"Group\")\n",
        "    raw_Groups = pd.concat([raw_GroupA, raw_GroupB])\n",
        "    imputed_GroupA = ImputedGroupData.get_group(item[0])\n",
        "    imputed_GroupB = ImputedGroupData.get_group(item[1]) \n",
        "    imputed_Groups = pd.concat([imputed_GroupA, imputed_GroupB]) \n",
        "    IDs = list(raw_Groups.index)\n",
        "    x1,y1,z1 = BetaDiversity(raw_Groups, IDs)\n",
        "    x2,y2,z2 = BetaDiversity(imputed_Groups, IDs)\n",
        "    y3, y4 = (y1+1)/2, (y2+1)/2\n",
        "    x3 = [metricID, item[0], item[1], y3, y4, (y4/y3)-1]\n",
        "    Results.append(x3)\n",
        "  results_df = pd.DataFrame.from_records(Results, columns = [\"Metric\", \"GroupA\", \"GroupB\", \"rawScore\", \"processedScore\", \"Improvement\"])\n",
        "  return results_df\n",
        "  #Results.to_csv(\"Improvements.tsv\", sep='\\t', index=False)\n",
        "\n",
        "\n",
        "def separationDistancesWithinGroups(GroupedData, ImputedGroupData):\n",
        "  distances =[]\n",
        "  for item in t:\n",
        "    raw_Group = GroupedData.get_group(item).set_index(\"Group\")\n",
        "    imputed_Group = ImputedGroupData.get_group(item) #.set_index(\"Group\")\n",
        "    pca = PCA()\n",
        "    f = []\n",
        "    g = []\n",
        "    scaled_data = preprocessing.scale(raw_Group)\n",
        "    components = pca.fit_transform(scaled_data)\n",
        "    a1 = pca.components_[0:3].T\n",
        "    b1 = pca.explained_variance_ratio_[0:3]\n",
        "    c1 = a1*b1\n",
        "    d1 = itertools.combinations(c1, 2)\n",
        "    for m in d1:\n",
        "      e1 = np.sqrt(np.sum((m[0]-m[1])**2, axis=0))\n",
        "      f.append(e1)\n",
        "    scaled_data = preprocessing.scale(imputed_Group)\n",
        "    components = pca.fit_transform(scaled_data)\n",
        "    a2 = pca.components_[0:3].T\n",
        "    b2 = pca.explained_variance_ratio_[0:3]\n",
        "    c2 = a2*b2\n",
        "    d2 = itertools.combinations(c2, 2)\n",
        "    for m in d2:\n",
        "      e2 = np.sqrt(np.sum((m[0]-m[1])**2, axis=0))\n",
        "      g.append(e2)\n",
        "    L, S = stats.levene(f, g)\n",
        "    statistics = [item, np.mean(f), np.std(f), np.var(f), np.median(f), stats.hmean(f).item(0), stats.gmean(f).item(0), min(f), max(f), np.mean(g), np.std(g), np.var(g), np.median(g), stats.hmean(g).item(0), stats.gmean(g).item(0), min(g), max(g), L, S]\n",
        "    distances.append(statistics)\n",
        "  distances_df = pd.DataFrame(distances, columns = [\"Group\", \"Mean\", \"StandardDeviation\", \"Variance\", \"Median\", \"HarmonicMean\", \"GeometricMean\", \"minimum\", \"maximum\", \"Mean\", \"StandardDeviation\", \"Variation\", \"Median\", \"HarmonicMean\", \"GeometricMean\", \"minimum\", \"maximum\", \"Levenes_W\", \"Levenes_P\"])\n",
        "  return distances_df\n",
        "#df.to_csv(\"Distances.tsv\", sep='\\t', header=True, index=False)\n",
        "\n",
        "\n",
        "from skbio.diversity import alpha as alpha_div\n",
        "\n",
        "def AlphaDiversityStatistics(rawDataset, imputedDataset):\n",
        "  #dataset_df1 = pd.DataFrame(columns=rawDataset.iloc[0])\n",
        "  dataset_df1 = rawDataset[1:]\n",
        "  dataset_df1.columns = rawDataset.iloc[0]\n",
        "  raw_Chao1 = dataset_df1.astype(float).apply(alpha_div.chao1, axis=0).apply(pd.to_numeric).sort_index()\n",
        "  raw_Shannon = dataset_df1.astype(float).apply(alpha_div.shannon, axis=0).apply(pd.to_numeric).sort_index()\n",
        "  raw_SimpsonD = dataset_df1.astype(float).apply(alpha_div.simpson, axis=0).apply(pd.to_numeric).sort_index()\n",
        "  raw_SimpsonE = dataset_df1.astype(float).apply(alpha_div.simpson_e, axis=0).apply(pd.to_numeric).sort_index()\n",
        "  imputed_Chao1 = imputedDataset.astype(float).apply(alpha_div.chao1, axis=0).apply(pd.to_numeric).sort_index()\n",
        "  imputed_Shannon = imputedDataset.astype(float).apply(alpha_div.shannon, axis=0).apply(pd.to_numeric).sort_index()\n",
        "  imputed_SimpsonD = imputedDataset.astype(float).apply(alpha_div.simpson, axis=0).apply(pd.to_numeric).sort_index()\n",
        "  imputed_SimpsonE = imputedDataset.astype(float).apply(alpha_div.simpson_e, axis=0).apply(pd.to_numeric).sort_index()\n",
        "  AlphaDObservations = [raw_Chao1, raw_Shannon, raw_SimpsonD, raw_SimpsonE, imputed_Chao1, imputed_Shannon, imputed_SimpsonD, imputed_SimpsonE]\n",
        "  AlphaDObservations = pd.concat(AlphaDObservations, axis=1).rename(columns={0:\"Chao1_raw\", 1:\"Shannon_raw\", 2:\"SimpsonsD_raw\", 3:\"SimpsonsE_raw\", 4:\"Chao1_processed\", 5:\"Shannon_processed\", 6:\"SimpsonsD_processed\", 7:\"SimpsonsE_processed\"})\n",
        "  AlphaDGroups = AlphaDObservations.groupby(\"Group\")\n",
        "  AlphaGroupStats = []    \n",
        "  for Group in Groups:\n",
        "    AlphaGroup = AlphaDGroups.get_group(Group)\n",
        "    raw_Chao1_group = AlphaGroup[\"Chao1_raw\"] \n",
        "    raw_Shannon_group = AlphaGroup[\"Shannon_raw\"]\n",
        "    raw_SimpsonsD_group = AlphaGroup[\"SimpsonsD_raw\"]  \n",
        "    raw_SimpsonsE_group = AlphaGroup[\"SimpsonsE_raw\"]\n",
        "    imputed_Chao1_group = AlphaGroup[\"Chao1_processed\"] \n",
        "    imputed_Shannon_group = AlphaGroup[\"Shannon_processed\"]\n",
        "    imputed_SimpsonsD_group = AlphaGroup[\"SimpsonsD_processed\"]  \n",
        "    imputed_SimpsonsE_group = AlphaGroup[\"SimpsonsE_processed\"]\n",
        "    results = PairwiseDescriptiveStats(raw_Chao1_group, imputed_Chao1_group)\n",
        "    results = [Group, \"Chao1\"] + results\n",
        "    AlphaGroupStats.append(results)\n",
        "    results = PairwiseDescriptiveStats(raw_Shannon_group, imputed_Shannon_group)\n",
        "    results = [Group, \"Shannon\"] + results\n",
        "    AlphaGroupStats.append(results)\n",
        "    results = PairwiseDescriptiveStats(raw_SimpsonsD_group, imputed_SimpsonsD_group)\n",
        "    results = [Group, \"SimpsonsD\"] + results\n",
        "    AlphaGroupStats.append(results)\n",
        "    results = PairwiseDescriptiveStats(raw_SimpsonsE_group, imputed_SimpsonsE_group)\n",
        "    results = [Group, \"SimpsonsE\"] + results\n",
        "    AlphaGroupStats.append(results)\n",
        "  AlphaGroupStats = pd.DataFrame(AlphaGroupStats, columns = [\"Group\", \"Metric\", \"Mean\", \"StandardDeviation\", \"Variance\", \"Median\", \"HarmonicMean\", \"GeometricMean\", \"minimum\", \"maximum\", \"Mean\", \"StandardDeviation\", \"Variation\", \"Median\", \"HarmonicMean\", \"GeometricMean\", \"minimum\", \"maximum\", \"Levenes_W\", \"Levenes_P\"])\n",
        "  return AlphaDObservations, AlphaGroupStats\n",
        "\n",
        "metric=\"optimal\"\n",
        "\n",
        "if metric==\"optimal\":\n",
        "  Scores = []\n",
        "  Metric_list=[\"mean\",\"median\",\"harmonic\",\"geometric\"]\n",
        "  for new_metric in Metric_list:\n",
        "    metricID = str(new_metric)\n",
        "    #metric == new_metric\n",
        "    if new_metric == \"mean\":\n",
        "      Metric = np.mean\n",
        "    if new_metric == \"median\":\n",
        "      Metric = np.median\n",
        "    if new_metric == \"geometric\":\n",
        "      Metric = stats.gmean\n",
        "    if new_metric == \"harmonic\":\n",
        "       Metric=stats.hmean\n",
        "    changeStats, imputed_df, transposed_imputed_df, grouped_imputed_df = iForest()\n",
        "    interGroup_separation = separationDistancesBetweenGroups(GroupedData, grouped_imputed_df)\n",
        "    intraGroup_separation = separationDistancesWithinGroups(GroupedData, grouped_imputed_df)\n",
        "    alphaDiversityObservations, alphaDiversityStatistics = AlphaDiversityStatistics(dataset_df, transposed_imputed_df)\n",
        "    changeStats.to_csv(metricID+\"_changeLog.tsv\", sep='\\t', header=True, index=False)\n",
        "    imputed_df.to_csv(metricID+\"_imputedDataset.tsv\", sep='\\t', header=True, index=True)\n",
        "    transposed_imputed_df.to_csv(metricID+\"_imputedDataset2.tsv\", sep='\\t', header=True, index=True)\n",
        "    interGroup_separation.to_csv(metricID+\"_interGroup_separation.tsv\", sep='\\t', header=True, index=False)\n",
        "    intraGroup_separation.to_csv(metricID+\"_intraGroup_separation.tsv\", sep='\\t', header=True, index=False)\n",
        "    alphaDiversityObservations.to_csv(metricID+\"_alphaDiversityObservations.tsv\", sep='\\t', header=True, index=True)\n",
        "    alphaDiversityStatistics.to_csv(metricID+\"_alphaDiversityStatistics.tsv\", sep='\\t', header=True, index=True)\n",
        "    #x = [metricID, stats.hmean(np.float64(interGroup_separation['Improvement'])).item(0)]\n",
        "    x = [metricID, stats.hmean(interGroup_separation['rawScore']).item(0), stats.hmean(interGroup_separation['processedScore']).item(0), (stats.hmean(interGroup_separation['processedScore']).item(0)/stats.hmean(interGroup_separation['rawScore']).item(0))-1, np.median(interGroup_separation['rawScore']), np.median(interGroup_separation['processedScore']), (np.median(interGroup_separation['processedScore'])/np.median(interGroup_separation['rawScore'])-1), np.mean(interGroup_separation['rawScore']), np.mean(interGroup_separation['processedScore']), np.mean(interGroup_separation['processedScore'])/np.mean(interGroup_separation['rawScore'])-1]\n",
        "    Scores.append(x)\n",
        "  Score_df = pd.DataFrame.from_records(Scores, columns = [\"Metric\", \"HarmonicRaw\", \"HarmonicProcessed\", \"HarmonicImprovement\", \"MedianRaw\", \"MedianProcessed\",\"MedianImprovement\", \"MeanRaw\", \"MeanProcessed\", \"MeanImprovement\"])\n",
        "  Score_df.to_csv(\"Scores.tsv\", sep='\\t', header=True, index=True)\n",
        "  print(\"The optimum metric is: \"+Score_df[Score_df.MeanImprovement == Score_df.MeanImprovement.max()]['Metric'][1])\n",
        "  optimal_viewing_metric = Score_df[Score_df.MeanImprovement == Score_df.MeanImprovement.max()]['Metric'][1]\n",
        "\n",
        "Score_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D5Av-ROxjst"
      },
      "source": [
        "#@title **Click here to change PCA settings**\n",
        "#@markdown The impution metric can be changed so differences can be viewed here. This metric can be any of **\"mean\"**, **\"median\"**, **\"geometric\"** (geometric mean), or **\"harmonic\"** (harmonic mean). By default, this is set to the optimum metric. The graph width and lengths can be changed using the \"graphWidth\" and \"graphHeight\" parameters.\n",
        "viewing_metric = optimal_viewing_metric\n",
        "graphWidth = 1000\n",
        "graphHeight = 1250"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tszKhb2-rZp"
      },
      "source": [
        "#@title **View PCA (original data)**\n",
        "#@markdown These graphs are fully interactive, they can be rotated and zoomed into, groups can also be hidden by clicking on their legend.\n",
        "InputData = dataset_df.T.set_index(\"Group\").sort_index().T\n",
        "Colours = InputData.columns.to_list()\n",
        "\n",
        "Components(InputData, graphWidth, graphHeight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AYf5Xd0_TNw",
        "cellView": "form"
      },
      "source": [
        "#@title **View PCA (processed data)**\n",
        "#@markdown These graphs are fully interactive, they can be rotated and zoomed into, groups can also be hidden by clicking on their legend.\n",
        "if viewing_metric == \"mean\":\n",
        "  ImputedData = pd.read_csv('/content/mean_imputedDataset.tsv', sep='\\t', index_col='Group').astype(np.float64).sort_index().T\n",
        "if viewing_metric == \"median\":\n",
        "  ImputedData = pd.read_csv('/content/median_imputedDataset.tsv', sep='\\t', index_col='Group').astype(np.float64).sort_index().T\n",
        "if viewing_metric == \"geometric\":\n",
        "  ImputedData = pd.read_csv('/content/geometric_imputedDataset.tsv', sep='\\t', index_col='Group').astype(np.float64).sort_index().T\n",
        "if viewing_metric == \"harmonic\":\n",
        "  ImputedData = pd.read_csv('/content/harmonic_imputedDataset.tsv', sep='\\t', index_col='Group').astype(np.float64).sort_index().T\n",
        "\n",
        "Colours = ImputedData.columns.to_list()\n",
        "\n",
        "Components(ImputedData, graphWidth, graphHeight)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}